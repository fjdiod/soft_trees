{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz = 10\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=bsz, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "batch_size=bsz, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    7     6     8     0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAB4CAYAAADi1gmcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f274d77b2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "classes = list(range(10))\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Leaf(nn.Module):\n",
    "    def __init__(self, i_size, o_size, h_size=128):\n",
    "        super(Leaf, self).__init__()\n",
    "        self.i2h = nn.Linear(i_size, h_size)\n",
    "        self.h2o = nn.Linear(h_size, o_size)\n",
    "        self.soft = nn.LogSoftmax(1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.is_leaf = True\n",
    "\n",
    "    def forward(self, features):\n",
    "        out = self.i2h(features)\n",
    "        out = self.relu(out)\n",
    "        out = self.h2o(out)\n",
    "        return self.soft(out)\n",
    "\n",
    "    def accum_probs(self, features, path_prob):\n",
    "        return [[path_prob, self.forward(features)]]\n",
    "\n",
    "class Node(nn.Module):\n",
    "    def __init__(self, i_size, o_size):\n",
    "        super(Node, self).__init__()\n",
    "        self.o_size = o_size\n",
    "        self.i_size = i_size\n",
    "        self.i2o = nn.Linear(i_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.is_leaf = False\n",
    "    \n",
    "    def build_tree(self, depth):\n",
    "        if depth < 0:\n",
    "            raise NotImplementedError\n",
    "        if depth - 1 > 0:\n",
    "            self.left = Node(self.i_size, self.o_size)\n",
    "            self.right = Node(self.i_size, self.o_size)\n",
    "            self.left.build_tree(depth - 1)\n",
    "            self.right.build_tree(depth - 1)\n",
    "        else:\n",
    "            self.left = Leaf(self.i_size, self.o_size)\n",
    "            self.right = Leaf(self.i_size, self.o_size)\n",
    "\n",
    "    def forward(self, features):\n",
    "        pr = self.prob_left(features)\n",
    "        return pr*self.left(features) + (1 - pr)*self.right(features)\n",
    "\n",
    "    def prob_left(self, features):\n",
    "        return self.sigmoid(self.i2o(features))\n",
    "\n",
    "    def accum_probs(self, features, path_prob):\n",
    "        res = []\n",
    "        p_l = self.sigmoid(self.i2o(features))\n",
    "        res_l = self.left.accum_probs(features, p_l*path_prob)\n",
    "        res_r = self.right.accum_probs(features, (1 - p_l)*path_prob)\n",
    "        res.extend(res_l)\n",
    "        res.extend(res_r)\n",
    "        return res\n",
    "        \n",
    "\n",
    "def tree_loss(path_probs, y_true):\n",
    "    loss = 0\n",
    "    criterion = nn.NLLLoss(reduce=False)\n",
    "    for p, pred in path_probs:\n",
    "        loss += (p.squeeze()*criterion(pred, y_true)).mean()\n",
    "        print(loss.shape)\n",
    "    return loss.mean()\n",
    "\n",
    "def tree_logloss(path_probs, y_true):\n",
    "    \"\"\"\n",
    "    Original loss from paper\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    criterion = nn.NLLLoss()\n",
    "    for p, pred in path_probs:\n",
    "        loss -= p*criterion(pred, y_true)\n",
    "    return -torch.log(loss.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Node(28*28, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.build_tree(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       " 0.8359\n",
       " 2.8088\n",
       " 2.8088\n",
       " 2.8215\n",
       " 2.2742\n",
       " 1.9432\n",
       " 0.9377\n",
       " 0.1740\n",
       "-0.2969\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       " 0.1740\n",
       " 2.4906\n",
       " 2.6815\n",
       " 1.6123\n",
       " 2.1723\n",
       " 2.7960\n",
       " 2.7960\n",
       " 2.7960\n",
       " 2.2360\n",
       " 1.3323\n",
       " 0.7722\n",
       " 2.0069\n",
       " 1.6759\n",
       " 0.1867\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.3351\n",
       "-0.3097\n",
       "-0.4242\n",
       "-0.3606\n",
       "-0.2969\n",
       " 0.8104\n",
       " 1.9178\n",
       " 2.7960\n",
       " 2.7960\n",
       " 2.7960\n",
       " 2.7960\n",
       " 2.7960\n",
       " 2.6306\n",
       " 0.1613\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.2460\n",
       " 0.4286\n",
       " 0.4286\n",
       " 0.4286\n",
       " 1.6887\n",
       " 2.7960\n",
       " 2.7960\n",
       " 0.6322\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.2969\n",
       " 2.5287\n",
       " 2.7960\n",
       " 1.2050\n",
       "-0.3351\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       " 1.6123\n",
       " 2.7960\n",
       " 2.1087\n",
       "-0.3224\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       " 0.7595\n",
       " 2.7069\n",
       " 2.7960\n",
       " 1.1923\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       " 2.7451\n",
       " 2.7960\n",
       " 2.2105\n",
       "-0.1060\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.3988\n",
       " 0.6831\n",
       " 2.7960\n",
       " 2.3124\n",
       "-0.1060\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.1315\n",
       " 1.3068\n",
       " 2.7960\n",
       " 2.7960\n",
       " 1.6759\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       " 0.7341\n",
       " 2.4269\n",
       " 2.7960\n",
       " 2.7960\n",
       " 2.7960\n",
       " 2.5797\n",
       " 1.2941\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.2206\n",
       " 2.7833\n",
       " 2.7960\n",
       " 2.7960\n",
       " 2.7960\n",
       " 1.5232\n",
       " 1.4087\n",
       " 2.5415\n",
       " 2.3378\n",
       "-0.0169\n",
       "-0.0551\n",
       "-0.3478\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.3733\n",
       " 1.2177\n",
       " 2.6178\n",
       " 2.7960\n",
       " 2.7960\n",
       "-0.3606\n",
       "-0.4242\n",
       " 0.1613\n",
       " 2.4778\n",
       " 2.4142\n",
       " 2.5542\n",
       " 0.0340\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       " 1.0523\n",
       " 2.7960\n",
       " 1.6123\n",
       "-0.4115\n",
       "-0.4242\n",
       "-0.4242\n",
       " 0.5304\n",
       " 2.4396\n",
       " 2.7960\n",
       " 0.0340\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       " 0.9759\n",
       " 2.7960\n",
       " 1.0141\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.3478\n",
       " 0.4922\n",
       "-0.2842\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       " 2.1596\n",
       " 2.7960\n",
       " 0.3649\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.2078\n",
       " 2.4269\n",
       " 2.6306\n",
       " 0.0595\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       " 0.0213\n",
       " 2.5669\n",
       " 2.1342\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       " 0.0849\n",
       " 2.6178\n",
       " 2.1342\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.2715\n",
       " 1.8032\n",
       " 1.1032\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "-0.4242\n",
       "[torch.FloatTensor of size 784]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.view((-1, 28*28))[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "prb = net.accum_probs(Variable(images.view((-1, 28*28))[0:2, :]), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 2.3804\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_loss(prb, Variable(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.04448346048593521 0.00022241730242967606\n",
      "1.02219557762146 0.08888920063320256\n",
      "1.451016902923584 0.08958627382096893\n",
      "0.7898892164230347 0.07377105898632635\n",
      "0.8192051649093628 0.09002668988283404\n",
      "1.0244543552398682 0.09055522648563055\n",
      "0.8202686309814453 0.06427575424821043\n",
      "1.0864510536193848 0.06934745590515376\n",
      "1.038622498512268 0.08273247984929184\n",
      "0.9751286506652832 0.08803182879798442\n",
      "0.8449207544326782 0.09709214693728882\n",
      "1.6900041103363037 0.07923221594566712\n",
      "1\n",
      "1.117004632949829 0.09148552033993837\n",
      "1.0373001098632812 0.053492266329180894\n",
      "0.9193560481071472 0.06770165810565232\n",
      "0.9085773229598999 0.07426115564112024\n",
      "0.9426153898239136 0.060279333355249494\n",
      "1.581984281539917 0.07996730690209006\n",
      "0.8262917399406433 0.04464407770685796\n",
      "1.4186687469482422 0.08468332829243082\n",
      "1.0184742212295532 0.06022437862151492\n",
      "1.783700704574585 0.07072012345957773\n",
      "1.1958775520324707 0.05676366136369324\n",
      "1.0159505605697632 0.08166369924033007\n",
      "2\n",
      "1.3204072713851929 0.08106961273033335\n",
      "1.20656418800354 0.054273743950489006\n",
      "1.2001416683197021 0.05138382116654611\n",
      "1.5498477220535278 0.08180975164873189\n",
      "0.5664224624633789 0.051097978407015035\n",
      "1.0965118408203125 0.05959568309531647\n",
      "1.313328504562378 0.053554794553404006\n",
      "0.8663555979728699 0.05170216243136792\n",
      "2.050429105758667 0.08129587434781911\n",
      "0.9895069003105164 0.06279321925190402\n",
      "1.0365841388702393 0.09404437325707476\n",
      "0.8960312008857727 0.05690750402854064\n",
      "3\n",
      "1.4265471696853638 0.06668195053788623\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-216-e2e54b9f2c35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccum_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mall_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(net.parameters())\n",
    "all_loss = np.zeros(200)\n",
    "criterion = nn.NLLLoss()\n",
    "for epoch in range(5):\n",
    "    print(epoch)\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        features, targets = batch\n",
    "        features = Variable(features.view(-1, 28*28))\n",
    "        targets = Variable(targets)\n",
    "        #out = net(features)\n",
    "        loss = tree_loss(net.accum_probs(features, 1), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        all_loss[i%200] = loss.data[0]\n",
    "        if i % 500 == 0:\n",
    "            print(all_loss.max(), all_loss.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2.337132692337036 0.01168566346168518\n",
      "2.3393688201904297 0.70941203083843\n",
      "1.3418190479278564 0.3813070855103433\n",
      "1.240387201309204 0.353357810145244\n",
      "1.5959804058074951 0.31500955848023293\n",
      "1.2646723985671997 0.2582339872000739\n",
      "1.3248506784439087 0.2482780652702786\n",
      "1.549913763999939 0.2536597382789478\n",
      "1.1887271404266357 0.25024431850295514\n",
      "1.5904780626296997 0.2372973468201235\n",
      "1.4080578088760376 0.2245685329241678\n",
      "1.1917880773544312 0.20229555137222632\n",
      "1.4389848709106445 0.20757214635959825\n",
      "1.5717592239379883 0.19943377160350792\n",
      "1.2524627447128296 0.19819177539437077\n",
      "1.2399612665176392 0.17059512886451558\n",
      "1.3004764318466187 0.1951107891072752\n",
      "1.2526800632476807 0.17038131895358674\n",
      "1.405723214149475 0.16615860196354335\n",
      "1.120182991027832 0.19178391175126308\n",
      "1.256012201309204 0.16591066210065036\n",
      "1.600950002670288 0.173624066633638\n",
      "1.495044231414795 0.14647242508857744\n",
      "1.1515161991119385 0.15268506557506042\n",
      "1.668564796447754 0.15179500662139617\n",
      "0.9392849802970886 0.13501581214077304\n",
      "0.994438648223877 0.14582072310411603\n",
      "1.3024874925613403 0.16812489391770213\n",
      "1.2526028156280518 0.16104403021628969\n",
      "2.090782642364502 0.15498800041357755\n",
      "1\n",
      "2.249655246734619 0.17349214131652843\n",
      "1.0166481733322144 0.11070077896758448\n",
      "1.1756541728973389 0.11778788677183911\n",
      "0.7878836393356323 0.1049861797504127\n",
      "1.190197229385376 0.10084869241138222\n",
      "0.8199559450149536 0.12384143023067735\n",
      "1.2783963680267334 0.10229497142849141\n",
      "1.1938849687576294 0.11119598269142443\n",
      "0.9072431325912476 0.11310906508471817\n",
      "1.49693763256073 0.11220718260694412\n",
      "1.5392417907714844 0.13216923019790555\n",
      "1.3014624118804932 0.09925999975246669\n",
      "1.3958823680877686 0.13049017116543837\n",
      "0.8362331390380859 0.11192860983122956\n",
      "1.2022597789764404 0.10416906210994056\n",
      "1.4045004844665527 0.1321995350914949\n",
      "0.9872671365737915 0.10715923404830391\n",
      "1.2150428295135498 0.12070791431877297\n",
      "0.7070863842964172 0.08775358264509123\n",
      "0.9247279167175293 0.11101535839035932\n",
      "1.7719978094100952 0.10617282761540991\n",
      "1.2982298135757446 0.11290516461915104\n",
      "1.0412086248397827 0.11625058454450482\n",
      "1.1608940362930298 0.10263639220320328\n",
      "1.4126402139663696 0.12701430193090346\n",
      "0.9020196795463562 0.1238744110269181\n",
      "0.9309577941894531 0.10307445798127446\n",
      "1.4845507144927979 0.09216137280731346\n",
      "1.2667077779769897 0.1063583835123427\n",
      "0.9360715746879578 0.1067571297995164\n",
      "2\n",
      "1.610906958580017 0.13621546054797362\n",
      "1.3921765089035034 0.08986252051625343\n",
      "0.9303730130195618 0.08107140277428698\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-246-72f4dd786c77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mall_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(net.parameters())\n",
    "all_loss = np.zeros(200)\n",
    "criterion = nn.NLLLoss()\n",
    "for epoch in range(5):\n",
    "    print(epoch)\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        features, targets = batch\n",
    "        features = Variable(features.view(-1, 28*28))\n",
    "        targets = Variable(targets)\n",
    "        out = net(features)\n",
    "        loss = criterion(out, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        all_loss[i%200] = loss.data[0]\n",
    "        if i % 200 == 0:\n",
    "            print(all_loss.max(), all_loss.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "nan nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-266-0f18e63191e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#out = net(features)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_logloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccum_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mall_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(net.parameters())\n",
    "all_loss = np.zeros(200)\n",
    "criterion = nn.NLLLoss()\n",
    "for epoch in range(5):\n",
    "    print(epoch)\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        features, targets = batch\n",
    "        features = Variable(features.view(-1, 28*28))\n",
    "        targets = Variable(targets)\n",
    "        #out = net(features)\n",
    "        loss = tree_logloss(net.accum_probs(features, 1), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        all_loss[i%200] = loss.data[0]\n",
    "        if i % 500 == 0:\n",
    "            print(all_loss.max(), all_loss.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "total = 0\n",
    "correct = 0\n",
    "for data in test_loader:\n",
    "    images, labels = data\n",
    "    features = Variable(images.view(-1, 28*28))\n",
    "    targets = Variable(labels)\n",
    "    outputs = net(features)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == targets.data).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9594"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAB4CAYAAADi1gmcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb2441c6390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for data in test_loader:\n",
    "    images, labels = data\n",
    "    features = Variable(images.view(-1, 28*28))\n",
    "    break\n",
    "imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "    0\n",
       "    6\n",
       "    0\n",
       "    2\n",
       "    1\n",
       "    7\n",
       "    0\n",
       "    0\n",
       "    5\n",
       "    0\n",
       "[torch.LongTensor of size 10x1]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(features).topk(1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Variable containing:\n",
       "   0.3698\n",
       "   0.2354\n",
       "   0.4157\n",
       "   0.3729\n",
       "   0.2251\n",
       "   0.2064\n",
       "   0.4087\n",
       "   0.2474\n",
       "   0.2635\n",
       "   0.4449\n",
       "  [torch.FloatTensor of size 10x1], Variable containing:\n",
       "  -2.3399 -2.1639 -2.3609 -2.2783 -2.1904 -2.4916 -2.0536 -2.3836 -2.4639 -2.3885\n",
       "  -2.1216 -2.4151 -2.4126 -2.6590 -1.6693 -2.6165 -2.2584 -2.4090 -2.7313 -2.2054\n",
       "  -2.4880 -2.4315 -2.4022 -2.4181 -1.8086 -2.3845 -2.6030 -2.5302 -2.6341 -1.7844\n",
       "  -2.0468 -2.6011 -2.1479 -2.4624 -1.6819 -2.7388 -2.4847 -2.4772 -2.7785 -2.1664\n",
       "  -2.3826 -2.1342 -2.5188 -2.6239 -1.7700 -2.5679 -2.3595 -2.4636 -2.3183 -2.1962\n",
       "  -2.3078 -2.1739 -2.2567 -2.5060 -2.0410 -2.2095 -2.4665 -2.6322 -2.4764 -2.1204\n",
       "  -2.3445 -2.3085 -2.1451 -2.3394 -2.0232 -2.3128 -2.3955 -2.4376 -2.6927 -2.1730\n",
       "  -2.2243 -2.3504 -2.1915 -2.2503 -2.1237 -2.3369 -2.4652 -2.4607 -2.5458 -2.1678\n",
       "  -2.1088 -2.3997 -2.3813 -2.4653 -1.7977 -2.5648 -2.4795 -2.3994 -2.4307 -2.2508\n",
       "  -2.2055 -2.3440 -2.4924 -2.2513 -1.9905 -2.3058 -2.5462 -2.3740 -2.7144 -2.0262\n",
       "  [torch.FloatTensor of size 10x10]], [Variable containing:\n",
       "   0.2023\n",
       "   0.1906\n",
       "   0.1621\n",
       "   0.2027\n",
       "   0.1740\n",
       "   0.1510\n",
       "   0.1896\n",
       "   0.2189\n",
       "   0.1629\n",
       "   0.2151\n",
       "  [torch.FloatTensor of size 10x1], Variable containing:\n",
       "  -2.2569 -2.5454 -2.5742 -2.3005 -2.1076 -2.5349 -2.1206 -2.7611 -1.9849 -2.1279\n",
       "  -2.4451 -2.6875 -2.3200 -2.3655 -2.1850 -2.1116 -2.2056 -2.9604 -2.6360 -1.6825\n",
       "  -2.3834 -2.6501 -2.3430 -2.7408 -2.0373 -2.2899 -1.9419 -2.5356 -2.3370 -2.0744\n",
       "  -2.4487 -2.3131 -2.2839 -2.4450 -1.9862 -2.4141 -2.3860 -2.5837 -2.3289 -2.0064\n",
       "  -2.2261 -2.3297 -2.4047 -2.5586 -1.8194 -2.6201 -2.3084 -2.5271 -2.3853 -2.1143\n",
       "  -2.1807 -2.3491 -2.3505 -2.7096 -1.8642 -2.5442 -2.2515 -2.9250 -2.2278 -2.0442\n",
       "  -2.6483 -2.6981 -2.3016 -2.0978 -2.2082 -2.0539 -2.2294 -2.7597 -2.4333 -1.9499\n",
       "  -2.2478 -2.2413 -2.4525 -2.5767 -2.0437 -2.4021 -2.3375 -2.4862 -2.2122 -2.1488\n",
       "  -2.4977 -2.5281 -2.4488 -2.2869 -2.1380 -2.2083 -1.8268 -3.1076 -2.3182 -2.1365\n",
       "  -2.4699 -2.7363 -2.6291 -2.4751 -2.0599 -2.2217 -2.0199 -2.6189 -2.3804 -1.8304\n",
       "  [torch.FloatTensor of size 10x10]], [Variable containing:\n",
       "   0.1500\n",
       "   0.3105\n",
       "   0.2761\n",
       "   0.2324\n",
       "   0.2885\n",
       "   0.3726\n",
       "   0.1900\n",
       "   0.2491\n",
       "   0.3863\n",
       "   0.2082\n",
       "  [torch.FloatTensor of size 10x1], Variable containing:\n",
       "  -2.4817 -2.3642 -2.1983 -1.9154 -2.3731 -2.7804 -2.2032 -2.2981 -2.0585 -2.6541\n",
       "  -2.2064 -2.4536 -2.2016 -2.3988 -2.6483 -2.4926 -2.2052 -2.0613 -1.9969 -2.5804\n",
       "  -2.3893 -2.3988 -2.2491 -2.2453 -2.7599 -2.7506 -2.4298 -2.0722 -1.8780 -2.1888\n",
       "  -2.0625 -2.3683 -2.3006 -2.3478 -2.4261 -2.5231 -2.3741 -2.1183 -2.1168 -2.5119\n",
       "  -2.1519 -2.4635 -2.1356 -2.4990 -2.5157 -2.7174 -2.1439 -2.1402 -2.0791 -2.3911\n",
       "  -2.1935 -2.4856 -2.1152 -2.3038 -2.6395 -2.2836 -2.3762 -2.1854 -2.1379 -2.4285\n",
       "  -2.5971 -2.3147 -2.3334 -2.3930 -2.8668 -2.2828 -2.2946 -1.8863 -2.2348 -2.1196\n",
       "  -2.2864 -2.5329 -2.0351 -2.3958 -2.5697 -2.4568 -2.3227 -2.2779 -2.1953 -2.0951\n",
       "  -2.0718 -2.4249 -2.0082 -2.2583 -2.6010 -2.4901 -2.2718 -2.2870 -2.2867 -2.4840\n",
       "  -2.3552 -2.3983 -2.4342 -2.0001 -2.7633 -2.2410 -2.2048 -2.1984 -2.0981 -2.5525\n",
       "  [torch.FloatTensor of size 10x10]], [Variable containing:\n",
       "   0.2779\n",
       "   0.2635\n",
       "   0.1462\n",
       "   0.1921\n",
       "   0.3125\n",
       "   0.2700\n",
       "   0.2118\n",
       "   0.2846\n",
       "   0.1874\n",
       "   0.1318\n",
       "  [torch.FloatTensor of size 10x1], Variable containing:\n",
       "  -2.0202 -2.7773 -2.2261 -3.0488 -2.0712 -2.1249 -2.8210 -2.3672 -2.6582 -1.7096\n",
       "  -2.2793 -2.5752 -2.2229 -2.5985 -1.9750 -2.0436 -2.6261 -2.2669 -2.6588 -2.0836\n",
       "  -2.4425 -2.5047 -2.0331 -3.1652 -1.9747 -2.3755 -2.9745 -2.3336 -2.3832 -1.6812\n",
       "  -2.3871 -2.4521 -2.1144 -2.6191 -2.0074 -2.0013 -2.8200 -2.4763 -2.4886 -2.0231\n",
       "  -2.2924 -2.8884 -1.9500 -2.8363 -2.3087 -2.3323 -2.6735 -2.1520 -2.1936 -1.9028\n",
       "  -2.1175 -2.9577 -2.2281 -2.8066 -2.3852 -2.4515 -2.2564 -2.1483 -2.1329 -1.9558\n",
       "  -2.3571 -2.5275 -1.9558 -2.5176 -2.2382 -1.9458 -2.5646 -2.4793 -2.6922 -2.0768\n",
       "  -1.9875 -2.6472 -2.1512 -2.6528 -2.2528 -2.2752 -2.2276 -2.3790 -2.5602 -2.1226\n",
       "  -2.2645 -2.6293 -2.2626 -2.8512 -2.1600 -2.1147 -2.4079 -2.2015 -2.5672 -1.9054\n",
       "  -2.5309 -2.1781 -1.7692 -2.5671 -2.4131 -2.2603 -2.6826 -2.6715 -2.5922 -1.8729\n",
       "  [torch.FloatTensor of size 10x10]]]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.accum_probs(features, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.5197  0.4803\n",
       "[torch.FloatTensor of size 1x2]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-0.4803\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(p[0][1], t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l = Leaf(20, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.7010  0.2990\n",
       " 0.4587  0.5413\n",
       " 0.6445  0.3555\n",
       " 0.6523  0.3477\n",
       " 0.5520  0.4480\n",
       " 0.5729  0.4271\n",
       " 0.4309  0.5691\n",
       " 0.6113  0.3887\n",
       " 0.5364  0.4636\n",
       " 0.4727  0.5273\n",
       " 0.5744  0.4256\n",
       " 0.4254  0.5746\n",
       " 0.6120  0.3880\n",
       " 0.3881  0.6119\n",
       " 0.4329  0.5671\n",
       " 0.6215  0.3785\n",
       " 0.5566  0.4434\n",
       " 0.4899  0.5101\n",
       " 0.5596  0.4404\n",
       " 0.4213  0.5787\n",
       " 0.5454  0.4546\n",
       " 0.6407  0.3593\n",
       " 0.7366  0.2634\n",
       " 0.5357  0.4643\n",
       " 0.4664  0.5336\n",
       " 0.4837  0.5163\n",
       " 0.4695  0.5305\n",
       " 0.3931  0.6069\n",
       " 0.4598  0.5402\n",
       " 0.5820  0.4180\n",
       " 0.5449  0.4551\n",
       " 0.5599  0.4401\n",
       " 0.5203  0.4797\n",
       " 0.4539  0.5461\n",
       " 0.4069  0.5931\n",
       " 0.4765  0.5235\n",
       " 0.6093  0.3907\n",
       " 0.5651  0.4349\n",
       " 0.5195  0.4805\n",
       " 0.7444  0.2556\n",
       " 0.4269  0.5731\n",
       " 0.4473  0.5527\n",
       " 0.5124  0.4876\n",
       " 0.4995  0.5005\n",
       " 0.7357  0.2643\n",
       " 0.5297  0.4703\n",
       " 0.5353  0.4647\n",
       " 0.5564  0.4436\n",
       " 0.4799  0.5201\n",
       " 0.4594  0.5406\n",
       " 0.4223  0.5777\n",
       " 0.6069  0.3931\n",
       " 0.5184  0.4816\n",
       " 0.2934  0.7066\n",
       " 0.4795  0.5205\n",
       " 0.4228  0.5772\n",
       " 0.4746  0.5254\n",
       " 0.4543  0.5457\n",
       " 0.4915  0.5085\n",
       " 0.7506  0.2494\n",
       " 0.3913  0.6087\n",
       " 0.5906  0.4094\n",
       " 0.6700  0.3300\n",
       " 0.4345  0.5655\n",
       " 0.4025  0.5975\n",
       " 0.5730  0.4270\n",
       " 0.4163  0.5837\n",
       " 0.5017  0.4983\n",
       " 0.4581  0.5419\n",
       " 0.5740  0.4260\n",
       " 0.5664  0.4336\n",
       " 0.4470  0.5530\n",
       " 0.4716  0.5284\n",
       " 0.4761  0.5239\n",
       " 0.4448  0.5552\n",
       " 0.5357  0.4643\n",
       " 0.4385  0.5615\n",
       " 0.5305  0.4695\n",
       " 0.5145  0.4855\n",
       " 0.5347  0.4653\n",
       " 0.4728  0.5272\n",
       " 0.4666  0.5334\n",
       " 0.3363  0.6637\n",
       " 0.6276  0.3724\n",
       " 0.4739  0.5261\n",
       " 0.5521  0.4479\n",
       " 0.5093  0.4907\n",
       " 0.4782  0.5218\n",
       " 0.5116  0.4884\n",
       " 0.5564  0.4436\n",
       " 0.4297  0.5703\n",
       " 0.3842  0.6158\n",
       " 0.3599  0.6401\n",
       " 0.4299  0.5701\n",
       " 0.5588  0.4412\n",
       " 0.4090  0.5910\n",
       " 0.3622  0.6378\n",
       " 0.5353  0.4647\n",
       " 0.4537  0.5463\n",
       " 0.6025  0.3975\n",
       "[torch.FloatTensor of size 100x2]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 20])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.5568\n",
       " 0.4157\n",
       " 0.6472\n",
       " 0.5052\n",
       " 0.7840\n",
       " 0.5970\n",
       " 0.5989\n",
       " 0.6032\n",
       " 0.5259\n",
       " 0.4826\n",
       " 0.5710\n",
       " 0.3354\n",
       " 0.6097\n",
       " 0.2600\n",
       " 0.4450\n",
       " 0.4455\n",
       " 0.6031\n",
       " 0.4423\n",
       " 0.6174\n",
       " 0.5324\n",
       " 0.4664\n",
       " 0.6138\n",
       " 0.3941\n",
       " 0.4854\n",
       " 0.3821\n",
       " 0.5882\n",
       " 0.6332\n",
       " 0.6062\n",
       " 0.2798\n",
       " 0.4175\n",
       " 0.4970\n",
       " 0.4680\n",
       " 0.4224\n",
       " 0.5263\n",
       " 0.3459\n",
       " 0.3255\n",
       " 0.5238\n",
       " 0.5932\n",
       " 0.4105\n",
       " 0.5355\n",
       " 0.6300\n",
       " 0.5799\n",
       " 0.5868\n",
       " 0.6435\n",
       " 0.6820\n",
       " 0.3809\n",
       " 0.5247\n",
       " 0.5251\n",
       " 0.4482\n",
       " 0.5785\n",
       " 0.3621\n",
       " 0.5591\n",
       " 0.4758\n",
       " 0.3217\n",
       " 0.5141\n",
       " 0.3364\n",
       " 0.6387\n",
       " 0.6046\n",
       " 0.4159\n",
       " 0.6508\n",
       " 0.2434\n",
       " 0.4416\n",
       " 0.7236\n",
       " 0.2498\n",
       " 0.3928\n",
       " 0.5419\n",
       " 0.6580\n",
       " 0.4069\n",
       " 0.4864\n",
       " 0.5594\n",
       " 0.5003\n",
       " 0.4538\n",
       " 0.4237\n",
       " 0.4355\n",
       " 0.4910\n",
       " 0.4696\n",
       " 0.6194\n",
       " 0.6092\n",
       " 0.5075\n",
       " 0.5751\n",
       " 0.5980\n",
       " 0.5513\n",
       " 0.3349\n",
       " 0.5950\n",
       " 0.4625\n",
       " 0.4790\n",
       " 0.5537\n",
       " 0.3851\n",
       " 0.6574\n",
       " 0.5526\n",
       " 0.3472\n",
       " 0.4050\n",
       " 0.5815\n",
       " 0.3393\n",
       " 0.3553\n",
       " 0.2930\n",
       " 0.1914\n",
       " 0.4552\n",
       " 0.1965\n",
       " 0.5975\n",
       "[torch.FloatTensor of size 100x1]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
