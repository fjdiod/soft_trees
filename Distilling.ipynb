{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz = 10\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=bsz, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "batch_size=bsz, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x, T=1):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x/T, dim=1)\n",
    "\n",
    "    \n",
    "class MLPNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "    def forward(self, x, T=1):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.log_softmax(x/T, dim=1)\n",
    "\n",
    "    \n",
    "def validate(model, batches_val):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    for batch in batches_val:\n",
    "        features, targets = batch\n",
    "        y_true += targets.tolist()\n",
    "        y_pred += model(Variable(features)).topk(1)[1].squeeze().data.tolist()\n",
    "    model.train()\n",
    "    return accuracy_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "def dist_loss(out, labels, teacher, T, alpha=1):\n",
    "    # KLD instead of cross-entropy\n",
    "    f = nn.NLLLoss()\n",
    "    return f(out, labels) + nn.KLDivLoss()(out, teacher)*alpha*T*T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0986"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.102"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 3\n",
       " 4\n",
       " 8\n",
       " 5\n",
       " 9\n",
       " 5\n",
       " 2\n",
       " 6\n",
       " 0\n",
       " 5\n",
       "[torch.LongTensor of size 10]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.1028\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.NLLLoss()(out, Variable(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.1548\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.NLLLoss()(model(Variable(images), T=1), Variable(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(Variable(images), T=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = torch.randn((10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.1539  0.0262 -1.1954  0.1588  0.7278  2.2301 -0.5055 -0.5056  0.1926  0.2100\n",
       " 1.1154  0.0551  0.7055  0.3275  1.2062 -0.0442 -0.2309 -0.2658  0.5998 -0.8250\n",
       " 1.2547  0.4871  0.5015 -1.1518  0.9102 -1.9922 -0.9561  0.9763  0.5878  0.4499\n",
       " 1.0981 -0.4271 -0.3846 -0.0518 -0.2813  0.2450  0.1398 -1.0106 -0.3075 -0.2331\n",
       " 0.2396 -1.1178 -0.8445 -0.1227 -0.0193 -1.4706 -0.7373  0.0439  0.2850  1.2408\n",
       "-0.3649  1.8554  0.0346  1.3589 -2.3692 -1.1487  0.2179 -0.9799 -0.8932 -0.1461\n",
       " 0.5386  1.1653 -1.5315 -1.1691 -0.0695 -0.4274  0.6384 -2.2958 -0.5693 -0.1638\n",
       " 0.8861  2.8362 -0.7377  0.3162  0.0122  0.2033  0.3851  0.5844 -0.3663 -1.4459\n",
       " 1.6428  0.1251 -1.1547 -1.7958 -0.5284  0.0292 -1.0785  0.1804  0.6491 -0.7858\n",
       "-0.2267  0.5243  0.7828 -0.3112  0.0115  0.1925 -0.7195 -0.9484  0.1149 -1.3008\n",
       "[torch.FloatTensor of size 10x10]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-4.1462 -2.8970 -1.7497 -1.0912 -3.6110 -3.0325 -4.7618 -1.7315 -2.3107 -2.8245\n",
       "-2.9200 -2.7526 -2.1662 -2.6158 -1.4263 -2.5391 -1.9825 -2.4884 -2.4612 -2.6684\n",
       "-4.0901 -3.0911 -2.9071 -2.0496 -3.1610 -2.1145 -2.3699 -3.8722 -0.8469 -3.0309\n",
       "-4.0136 -3.9463 -4.2655 -2.0009 -3.4587 -0.8159 -3.3900 -2.5985 -2.7108 -1.8015\n",
       "-4.7107 -4.5514 -3.5822 -2.8478 -2.2337 -3.7733 -7.2636 -1.8880 -2.6121 -0.6179\n",
       "-2.8180 -3.8386 -3.4167 -2.1056 -2.7148 -1.1374 -2.7607 -2.6737 -3.0516 -1.6207\n",
       "-3.3092 -3.5563 -0.6738 -1.7274 -4.7652 -4.0080 -4.2577 -2.7040 -2.1067 -4.0201\n",
       "-2.1608 -3.8429 -2.7477 -3.5961 -2.2231 -2.7197 -0.7823 -3.8699 -2.4188 -3.4923\n",
       "-1.5345 -2.5138 -2.2990 -2.9358 -2.4746 -2.6045 -1.9151 -2.5914 -2.4913 -2.4437\n",
       "-5.1462 -7.8088 -6.5441 -2.5508 -5.7172 -0.2723 -3.0963 -6.2423 -3.4774 -2.6397\n",
       "[torch.FloatTensor of size 10x10]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.KLDivLoss()(model(Variable(images), T=1), Variable(out.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.2022\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_loss(model(Variable(images), T=1) , Variable(labels), Variable(out.data), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Variable data has to be a tensor, but got Variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-0a26acda9526>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Variable data has to be a tensor, but got Variable"
     ]
    }
   ],
   "source": [
    "Variable(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.NLLLoss()\n",
    "n_epochs = 5\n",
    "print_every = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0\n",
      "0.0786\n",
      "2.519871234893799 0.8577690431810916\n",
      "1.9651288986206055 0.4190021920343861\n",
      "2.404747247695923 0.33846910459490026\n",
      "1.8743788003921509 0.32285841274261473\n",
      "2.3775455951690674 0.2929125738083385\n",
      "3.9713432788848877 0.26570564077064046\n",
      "EPOCH: 1\n",
      "0.9756\n",
      "2.476496696472168 0.24641338495412493\n",
      "2.035698652267456 0.2383810167213669\n",
      "3.515939235687256 0.2246983324928442\n",
      "2.2007484436035156 0.219863897053001\n",
      "2.840733051300049 0.22937279218275217\n",
      "2.179814577102661 0.21133869342284742\n",
      "EPOCH: 2\n",
      "0.9806\n",
      "1.862501859664917 0.20715391832842578\n",
      "2.0298733711242676 0.20375876118706945\n",
      "1.8114639520645142 0.20473041690396349\n",
      "2.1009151935577393 0.20983489174311398\n",
      "1.7443807125091553 0.18402129105722997\n",
      "2.7196121215820312 0.20471289938790868\n",
      "EPOCH: 3\n",
      "0.983\n",
      "2.0051004886627197 0.19047316831730132\n",
      "2.773045301437378 0.1960992682632641\n",
      "3.0417144298553467 0.19267630136001845\n",
      "1.84195876121521 0.18145983694877213\n",
      "2.0361733436584473 0.17788843801156326\n",
      "2.326615810394287 0.18020267077093013\n",
      "EPOCH: 4\n",
      "0.9852\n",
      "2.596776247024536 0.16723552431371355\n",
      "1.6592810153961182 0.17326692616660147\n",
      "2.032378673553467 0.17189777309154305\n",
      "2.7002010345458984 0.17122371526498317\n",
      "1.9229837656021118 0.17072560629591316\n",
      "1.5594106912612915 0.17249606241153015\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "model = Net()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "for epoch in range(n_epochs):\n",
    "    print('EPOCH: {}'.format(epoch))\n",
    "    val_loss += [validate(model, test_loader)]\n",
    "    print(val_loss[-1])\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        features, targets = batch\n",
    "        features = Variable(features)\n",
    "        targets = Variable(targets)\n",
    "        out = model(features)\n",
    "        loss = criterion(out, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += [loss.data[0]]\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print(np.max(train_loss), np.mean(train_loss))\n",
    "            train_loss = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Train MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0\n",
      "0.0868\n",
      "2.4339194297790527 1.281710759960115\n",
      "2.5909934043884277 1.154912054359913\n",
      "2.312778949737549 1.1212260313797742\n",
      "2.7982451915740967 1.1218074560388922\n",
      "3.1012041568756104 1.0922972073554993\n",
      "2.335113763809204 1.0780489094108343\n",
      "EPOCH: 1\n",
      "0.6513\n",
      "2.244574785232544 1.0718755698390305\n",
      "2.707176685333252 1.041054245964624\n",
      "2.437919855117798 1.0873546012192965\n",
      "3.0367701053619385 1.0623042701147496\n",
      "2.3025853633880615 1.0523403420951218\n",
      "2.5725345611572266 1.0680838023247197\n",
      "EPOCH: 2\n",
      "0.6588\n",
      "2.3025853633880615 1.0417463421588764\n",
      "2.4176878929138184 1.0324794520940632\n",
      "2.245634078979492 1.0593762391423807\n",
      "2.5138306617736816 1.032323133953847\n",
      "3.0049571990966797 1.0340886754356324\n",
      "2.774027109146118 1.0594505604356528\n",
      "EPOCH: 3\n",
      "0.6507\n",
      "2.2482972145080566 0.9655954087497229\n",
      "2.1898789405822754 0.828860950259259\n",
      "2.588641405105591 0.8287117438032292\n",
      "2.163239002227783 0.8238641455876641\n",
      "2.395068407058716 0.816756155544892\n",
      "2.046461820602417 0.8145547002084204\n",
      "EPOCH: 4\n",
      "0.7492\n",
      "2.129333734512329 0.8245511711740401\n",
      "2.0778965950012207 0.8088409069133923\n",
      "2.6978611946105957 0.8012612408763089\n",
      "2.225705146789551 0.7856495016154368\n",
      "2.1744911670684814 0.7875671116658487\n",
      "2.1297948360443115 0.8258959797095158\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "model_mlp = MLPNet()\n",
    "optimizer = optim.Adam(model_mlp.parameters())\n",
    "for epoch in range(n_epochs):\n",
    "    print('EPOCH: {}'.format(epoch))\n",
    "    val_loss += [validate(model_mlp, test_loader)]\n",
    "    print(val_loss[-1])\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        features, targets = batch\n",
    "        features = Variable(features)\n",
    "        targets = Variable(targets)\n",
    "        out = model_mlp(features)\n",
    "        loss = criterion(out, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += [loss.data[0]]\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print(np.max(train_loss), np.mean(train_loss))\n",
    "            train_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
