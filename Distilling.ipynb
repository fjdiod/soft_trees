{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz = 10\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=bsz, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "batch_size=bsz, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class MLPNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "    def forward(self, x, T=1):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "    \n",
    "def validate(model, batches_val):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    for batch in batches_val:\n",
    "        features, targets = batch\n",
    "        y_true += targets.tolist()\n",
    "        y_pred += model(Variable(features)).topk(1)[1].squeeze().data.tolist()\n",
    "    model.train()\n",
    "    return accuracy_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "def distill_loss(out, labels, teacher, T, alpha=1):\n",
    "    # KLD instead of cross-entropy\n",
    "    f = nn.NLLLoss()\n",
    "    out_sm = F.log_softmax(out/T, dim=1)\n",
    "    teacher_sm = F.log_softmax(out/T, dim=1)\n",
    "    term1 = f(F.log_softmax(out, dim=1), labels)\n",
    "    term2 = nn.KLDivLoss()(out_sm, teacher_sm.detach())*alpha*T*T\n",
    "    return term1 + term2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0986"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.102"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-0.1736\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.KLDivLoss()(out, teacher.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "you can only change requires_grad flags of leaf variables. If you want to use a computed variable in a subgraph that doesn't require differentiation use var_no_grad = var.detach().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-ee5ef7ed2a9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mteacher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: you can only change requires_grad flags of leaf variables. If you want to use a computed variable in a subgraph that doesn't require differentiation use var_no_grad = var.detach()."
     ]
    }
   ],
   "source": [
    "teacher.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.1548\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.NLLLoss()(model(Variable(images), T=1), Variable(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(Variable(images), T=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = torch.randn((10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.1539  0.0262 -1.1954  0.1588  0.7278  2.2301 -0.5055 -0.5056  0.1926  0.2100\n",
       " 1.1154  0.0551  0.7055  0.3275  1.2062 -0.0442 -0.2309 -0.2658  0.5998 -0.8250\n",
       " 1.2547  0.4871  0.5015 -1.1518  0.9102 -1.9922 -0.9561  0.9763  0.5878  0.4499\n",
       " 1.0981 -0.4271 -0.3846 -0.0518 -0.2813  0.2450  0.1398 -1.0106 -0.3075 -0.2331\n",
       " 0.2396 -1.1178 -0.8445 -0.1227 -0.0193 -1.4706 -0.7373  0.0439  0.2850  1.2408\n",
       "-0.3649  1.8554  0.0346  1.3589 -2.3692 -1.1487  0.2179 -0.9799 -0.8932 -0.1461\n",
       " 0.5386  1.1653 -1.5315 -1.1691 -0.0695 -0.4274  0.6384 -2.2958 -0.5693 -0.1638\n",
       " 0.8861  2.8362 -0.7377  0.3162  0.0122  0.2033  0.3851  0.5844 -0.3663 -1.4459\n",
       " 1.6428  0.1251 -1.1547 -1.7958 -0.5284  0.0292 -1.0785  0.1804  0.6491 -0.7858\n",
       "-0.2267  0.5243  0.7828 -0.3112  0.0115  0.1925 -0.7195 -0.9484  0.1149 -1.3008\n",
       "[torch.FloatTensor of size 10x10]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-4.1462 -2.8970 -1.7497 -1.0912 -3.6110 -3.0325 -4.7618 -1.7315 -2.3107 -2.8245\n",
       "-2.9200 -2.7526 -2.1662 -2.6158 -1.4263 -2.5391 -1.9825 -2.4884 -2.4612 -2.6684\n",
       "-4.0901 -3.0911 -2.9071 -2.0496 -3.1610 -2.1145 -2.3699 -3.8722 -0.8469 -3.0309\n",
       "-4.0136 -3.9463 -4.2655 -2.0009 -3.4587 -0.8159 -3.3900 -2.5985 -2.7108 -1.8015\n",
       "-4.7107 -4.5514 -3.5822 -2.8478 -2.2337 -3.7733 -7.2636 -1.8880 -2.6121 -0.6179\n",
       "-2.8180 -3.8386 -3.4167 -2.1056 -2.7148 -1.1374 -2.7607 -2.6737 -3.0516 -1.6207\n",
       "-3.3092 -3.5563 -0.6738 -1.7274 -4.7652 -4.0080 -4.2577 -2.7040 -2.1067 -4.0201\n",
       "-2.1608 -3.8429 -2.7477 -3.5961 -2.2231 -2.7197 -0.7823 -3.8699 -2.4188 -3.4923\n",
       "-1.5345 -2.5138 -2.2990 -2.9358 -2.4746 -2.6045 -1.9151 -2.5914 -2.4913 -2.4437\n",
       "-5.1462 -7.8088 -6.5441 -2.5508 -5.7172 -0.2723 -3.0963 -6.2423 -3.4774 -2.6397\n",
       "[torch.FloatTensor of size 10x10]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.KLDivLoss()(model(Variable(images), T=1), Variable(out.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.2022\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_loss(model(Variable(images), T=1) , Variable(labels), Variable(out.data), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Variable data has to be a tensor, but got Variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-0a26acda9526>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Variable data has to be a tensor, but got Variable"
     ]
    }
   ],
   "source": [
    "Variable(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.NLLLoss()\n",
    "n_epochs = 5\n",
    "print_every = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0\n",
      "0.088\n",
      "3.3430638313293457 0.8510436536222696\n",
      "2.7960424423217773 0.42013787904707717\n",
      "2.2679131031036377 0.34147757189301775\n",
      "2.175872564315796 0.31622400746087076\n",
      "2.2482104301452637 0.2827621094827773\n",
      "1.7645008563995361 0.2706581936685834\n",
      "EPOCH: 1\n",
      "0.9732\n",
      "2.3818540573120117 0.2570315435518278\n",
      "1.991175889968872 0.23137446979264495\n",
      "1.9281914234161377 0.23398442622000584\n",
      "2.214446783065796 0.228740786645707\n",
      "2.1853623390197754 0.23931755483630696\n",
      "4.643822193145752 0.21680072463536634\n",
      "EPOCH: 2\n",
      "0.9812\n",
      "2.310992956161499 0.20182792831095867\n",
      "1.740277886390686 0.2051407995767513\n",
      "1.8288999795913696 0.2029360617431521\n",
      "2.1287429332733154 0.19039480391783217\n",
      "1.7041432857513428 0.19860290015446663\n",
      "2.1531929969787598 0.19763636078608943\n",
      "EPOCH: 3\n",
      "0.9823\n",
      "2.0464532375335693 0.20229402248740372\n",
      "2.5500807762145996 0.17720872070675522\n",
      "2.536996603012085 0.19284904746242681\n",
      "2.1391892433166504 0.1734705840072893\n",
      "1.823582649230957 0.18703137124369096\n",
      "1.7932226657867432 0.19080921686431976\n",
      "EPOCH: 4\n",
      "0.9824\n",
      "1.7043308019638062 0.17624145606076855\n",
      "1.894330620765686 0.17114954308810412\n",
      "2.235199451446533 0.17260829130101774\n",
      "1.8584163188934326 0.16305027056967084\n",
      "1.7355749607086182 0.15721027177123686\n",
      "1.8553497791290283 0.16942519198744413\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "model = Net()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "for epoch in range(n_epochs):\n",
    "    print('EPOCH: {}'.format(epoch))\n",
    "    val_loss += [validate(model, test_loader)]\n",
    "    print(val_loss[-1])\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        features, targets = batch\n",
    "        features = Variable(features)\n",
    "        targets = Variable(targets)\n",
    "        out = model(features)\n",
    "        loss = criterion(F.log_softmax(out, dim=1), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += [loss.data[0]]\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print(np.max(train_loss), np.mean(train_loss))\n",
    "            train_loss = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Train MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0\n",
      "0.106\n",
      "2.6320433616638184 1.605462523818016\n",
      "2.6214921474456787 1.526390792965889\n",
      "2.5565125942230225 1.5039487244188785\n",
      "2.7537596225738525 1.4267951794117688\n",
      "2.78882098197937 1.338061695612967\n",
      "2.969398021697998 1.2950184574052692\n",
      "EPOCH: 1\n",
      "0.5508\n",
      "2.7672393321990967 1.2906241402179002\n",
      "2.3240065574645996 1.2787590429186821\n",
      "2.6269984245300293 1.2688658639788628\n",
      "2.5018692016601562 1.2613810243895278\n",
      "2.7148659229278564 1.2835158790051937\n",
      "2.7108378410339355 1.2717669401466847\n",
      "EPOCH: 2\n",
      "0.5599\n",
      "2.3090381622314453 1.2524490236639976\n",
      "2.567070960998535 1.2493149854838848\n",
      "2.50915789604187 1.2678505235612392\n",
      "2.518629312515259 1.259647961884737\n",
      "2.3025853633880615 1.2522857138365506\n",
      "2.6223530769348145 1.1735659117400645\n",
      "EPOCH: 3\n",
      "0.6464\n",
      "2.3260626792907715 1.0507244988897582\n",
      "2.4671120643615723 1.042844795199111\n",
      "2.797755718231201 1.042956533075543\n",
      "2.5100579261779785 1.0464946614067303\n",
      "2.163175344467163 1.033682730421424\n",
      "2.5838637351989746 1.019644301354885\n",
      "EPOCH: 4\n",
      "0.6603\n",
      "2.8707222938537598 1.041290293518745\n",
      "2.16021728515625 1.0223321169186383\n",
      "2.3799052238464355 1.0186963304285892\n",
      "2.172685384750366 1.0176512063506962\n",
      "2.1938281059265137 1.034752689001354\n",
      "2.364133834838867 1.0183450850769877\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "model_mlp = MLPNet()\n",
    "optimizer = optim.Adam(model_mlp.parameters())\n",
    "for epoch in range(n_epochs):\n",
    "    print('EPOCH: {}'.format(epoch))\n",
    "    val_loss += [validate(model_mlp, test_loader)]\n",
    "    print(val_loss[-1])\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        features, targets = batch\n",
    "        features = Variable(features)\n",
    "        targets = Variable(targets)\n",
    "        out = model_mlp(features)\n",
    "        loss = criterion(F.log_softmax(out, dim=1), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += [loss.data[0]]\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print(np.max(train_loss), np.mean(train_loss))\n",
    "            train_loss = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Train MLP with distillation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0\n",
      "0.1053\n",
      "2.4506101608276367 1.0127332982420922\n",
      "2.4707589149475098 0.8858727484801784\n",
      "2.3847155570983887 0.8652417615484447\n",
      "2.117615222930908 0.8130836763428524\n",
      "2.5091567039489746 0.8332938201297074\n",
      "2.2437922954559326 0.8160942573484499\n",
      "EPOCH: 1\n",
      "0.6932\n",
      "2.991323471069336 0.8001047033295036\n",
      "2.4121506214141846 0.7835619333037175\n",
      "2.2176852226257324 0.7993163074050099\n",
      "2.1255030632019043 0.770697611205047\n",
      "2.687572717666626 0.7975997999751707\n",
      "1.8874967098236084 0.7750216433053138\n",
      "EPOCH: 2\n",
      "0.6934\n",
      "2.0765976905822754 0.7638222025652358\n",
      "2.196305751800537 0.76588880770572\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "model_mlp = MLPNet()\n",
    "optimizer = optim.Adam(model_mlp.parameters())\n",
    "for epoch in range(n_epochs):\n",
    "    print('EPOCH: {}'.format(epoch))\n",
    "    val_loss += [validate(model_mlp, test_loader)]\n",
    "    print(val_loss[-1])\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        features, targets = batch\n",
    "        features = Variable(features)\n",
    "        targets = Variable(targets)\n",
    "        out = model_mlp(features)\n",
    "        teacher = model(features)\n",
    "        loss = distill_loss(out, targets, teacher, 5)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += [loss.data[0]]\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print(np.max(train_loss), np.mean(train_loss))\n",
    "            train_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 2.3101\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distill_loss(Variable(out.data), targets, Variable(teacher.data), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-1.7975\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.KLDivLoss()(out, teacher.detach())*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
